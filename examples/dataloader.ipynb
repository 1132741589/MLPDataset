{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "from data_lp import LPDataset,get_image_transform\n",
    "from vocab import Vocabulary\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "\n",
    "\n",
    "def default_collate_fn(data, caption_lim = 512):\n",
    "  # Sort a data list by sentence length\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, spoken_target, ocr_target, index, img_ids = zip(*data)\n",
    "    # Merge images (convert tuple of 3D tensor to 4D tensor)\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    cap_lengths = torch.tensor([len(cap) if len(cap) <= caption_lim else caption_lim for cap in spoken_target])\n",
    "    spoken_output = torch.zeros(len(spoken_target), caption_lim).long()\n",
    "    \n",
    "\n",
    "    for i, cap in enumerate(spoken_target):\n",
    "      end = cap_lengths[i]\n",
    "      if end <= caption_lim:\n",
    "        spoken_output[i, :end] = cap[:end]\n",
    "      else:\n",
    "        cap_lengths[i] = caption_lim\n",
    "        spoken_output[i, :end] = cap[:caption_lim]\n",
    "    return images, spoken_output, ocr_target, cap_lengths, index, img_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define speaker\n",
    "sp = 'psy-1'\n",
    "\n",
    "#define where data is downloaded - this is the zip file that you download \n",
    "root_data_dir = '/projects/dataset_processed/dongwonl/data/'\n",
    "sp_data_dir = '{}/{}'.format(root_data_dir, sp)\n",
    "\n",
    "#define where the training script with data_lp.py is in\n",
    "src_dr = '/work/dongwonl/lecture_project/kumon'\n",
    "\n",
    "#define where the vocab folder is in \n",
    "target_vocab_path = './vocab/%s_vocab.pkl' % sp\n",
    "vocab = pickle.load(open(target_vocab_path, 'rb'))\n",
    "\n",
    "#read jsons \n",
    "with open(\"{}/{}/{}_figs.json\".format(root_data_dir, sp,sp), 'r') as f:\n",
    " fig_json = json.loads(f.read())\n",
    "\n",
    "with open(\"{}/{}/{}.json\".format(root_data_dir, sp,sp), 'r') as j:\n",
    " cap_json = json.loads(j.read())\n",
    "\n",
    "with open(\"{}/{}/{}_capfig.json\".format(root_data_dir, sp,sp), 'r') as c:\n",
    " connect_json = json.loads(c.read())\n",
    "\n",
    "#load data\n",
    "transform = get_image_transform()\n",
    "wemb_type = 'bert'\n",
    "dataset = LPDataset(cap_json, fig_json, connect_json, vocab, sp_data_dir, wemb_type, transform)\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                        batch_size=2,\n",
    "                                        shuffle=True,\n",
    "                                        pin_memory=True,\n",
    "                                        num_workers=1,\n",
    "                                        collate_fn = default_collate_fn\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample Loop \n",
    "\n",
    "images: normalized images of figures (3 x 224 x 224)\n",
    "spoken_output: word tokens for spoken language (glove or bert)\n",
    "ocr_target: word tokens for ocr on-slide text (glove or bert)\n",
    "cap_lengths: length of captions\n",
    "index: number image ids\n",
    "img_ids: string image ids\n",
    "\n",
    "'''\n",
    "\n",
    "for itr, data in enumerate(tqdm(loader)):\n",
    "    images, spoken_output, ocr_target, cap_lengths, index, img_ids = data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
